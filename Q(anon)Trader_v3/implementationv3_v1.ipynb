{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1cf939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "import yfinance as yf\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "\n",
    "from src.agent import DoubleQLearningAgent\n",
    "from src.environment import make_env, StockTradingEnvironment\n",
    "from src.utils import save_pickle, load_pickle, run_learned_policy, run_multiple_episodes, plot_training_progress\n",
    "from src.data import fetch_stock_data, prepare_data, debug_dataframe\n",
    "from src.ensemble import AgentEnsemble\n",
    "from src.visualize import visualize_signals, plot_learning_curves\n",
    "from src.metrics import calculate_metrics\n",
    "\n",
    "# Set up logging and GPU/CPU configuration\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    device = '/GPU:0'\n",
    "    logging.info(\"Using GPU for training\")\n",
    "else:\n",
    "    device = '/CPU:0'\n",
    "    logging.info(\"Using CPU for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247295f8",
   "metadata": {},
   "source": [
    "#### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169c51c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data = fetch_stock_data('AAPL', '2019-06-20', '2024-06-21', 'AAPL_data.csv')\n",
    "debug_dataframe(stock_data, \"Raw Stock Data\", save_csv=True, output_file=\"raw_stock_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd4c31",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a633be93",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53bc28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data, scaler, optimal_features, stock_data = prepare_data('AAPL', '2019-06-20', '2024-06-21', n_select=15, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a40cecf",
   "metadata": {},
   "source": [
    "#### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d38c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_envs = 3\n",
    "file_path = 'preprocessed_data.csv'\n",
    "envs = gym.vector.SyncVectorEnv([lambda: make_env(file_path, number_of_days_to_consider=20, n_select=15) for _ in range(num_envs)])\n",
    "\n",
    "# Verify the observation space\n",
    "print(f\"Observation space: {envs.single_observation_space}\")\n",
    "print(f\"Action space: {envs.single_action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87349c4",
   "metadata": {},
   "source": [
    "#### Agent Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed4414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_agents = 5\n",
    "learning_rate = 0.001\n",
    "discount_factor = 0.95\n",
    "ensemble = AgentEnsemble(num_agents, \n",
    "                         observation_space=envs.single_observation_space, \n",
    "                         action_space=envs.single_action_space,\n",
    "                         learning_rate=learning_rate, \n",
    "                         discount_factor=discount_factor,\n",
    "                         epsilon=1.0, \n",
    "                         epsilon_min=0.01, \n",
    "                         epsilon_decay=0.995,\n",
    "                         buffer_size=10000, \n",
    "                         batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966e83c2",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5821cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "replay_frequency = 32  # Increased from 4 to 32\n",
    "reward_across_episodes = []\n",
    "epsilons_across_episodes = []\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 100\n",
    "min_delta = 0.001\n",
    "window_size = 20\n",
    "\n",
    "# Initialize variables for early stopping\n",
    "best_reward = float('-inf')\n",
    "wait = 0\n",
    "reward_window = deque(maxlen=window_size)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    observations, _ = envs.reset()\n",
    "    terminated = np.array([False] * num_envs)\n",
    "    truncated = np.array([False] * num_envs)\n",
    "    episode_rewards = np.zeros(num_envs)\n",
    "    \n",
    "    step_count = 0\n",
    "    while not np.all(terminated) and not np.all(truncated):\n",
    "        actions = [ensemble.act({\"observation\": obs}) for obs in observations]\n",
    "        next_observations, rewards, terminated, truncated, _ = envs.step(actions)\n",
    "        episode_rewards += rewards\n",
    "        \n",
    "        # Vectorized update\n",
    "        states, actions, rewards, next_states, dones, td_errors = ensemble.update(\n",
    "            tf.convert_to_tensor(observations, dtype=tf.float32),\n",
    "            tf.convert_to_tensor(actions, dtype=tf.int32),\n",
    "            tf.convert_to_tensor(rewards, dtype=tf.float32),\n",
    "            tf.convert_to_tensor(next_observations, dtype=tf.float32),\n",
    "            tf.convert_to_tensor(terminated | truncated, dtype=tf.bool)\n",
    "        )\n",
    "        \n",
    "        # Update memories outside of the TensorFlow graph\n",
    "        ensemble.update_memories(states, actions, rewards, next_states, dones, td_errors)\n",
    "        \n",
    "        observations = next_observations\n",
    "\n",
    "        # Perform experience replay less frequently\n",
    "        if step_count % replay_frequency == 0:\n",
    "            ensemble.replay()\n",
    "\n",
    "        step_count += 1\n",
    "\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    reward_across_episodes.append(mean_reward)\n",
    "    epsilons_across_episodes.append(ensemble.agents[0].epsilon)  # Assuming all agents have the same epsilon\n",
    "\n",
    "    ensemble.decay_epsilon()\n",
    "\n",
    "    # Early stopping check\n",
    "    reward_window.append(mean_reward)\n",
    "    if len(reward_window) == window_size:\n",
    "        current_reward = np.mean(reward_window)\n",
    "        if current_reward > best_reward + min_delta:\n",
    "            best_reward = current_reward\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping triggered at episode {episode + 1}\")\n",
    "                break\n",
    "\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        logging.info(f\"Episode {episode + 1}/{num_episodes} - Mean Reward: {mean_reward} - Epsilon: {ensemble.agents[0].epsilon}\")\n",
    "\n",
    "print(f\"Training completed after {episode + 1} episodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60898cae",
   "metadata": {},
   "source": [
    "#### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e78d91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward_learned_policy = run_multiple_episodes(envs, ensemble, num_episodes=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0f3e0e",
   "metadata": {},
   "source": [
    "#### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c959a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(reward_across_episodes, epsilons_across_episodes)\n",
    "plot_training_progress(reward_across_episodes, epsilons_across_episodes, total_reward_learned_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07fda3f",
   "metadata": {},
   "source": [
    "### Single Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25de7bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_trading_environment = StockTradingEnvironment('./AAPL_data.csv', number_of_days_to_consider=30)\n",
    "stock_trading_environment.train = False\n",
    "total_reward = run_learned_policy(stock_trading_environment, ensemble, verbose=True)\n",
    "print(f\"Total reward on test environment: {total_reward}\")\n",
    "\n",
    "obs, _ = stock_trading_environment.reset()\n",
    "terminated, truncated = False, False\n",
    "predictions = []\n",
    "while not terminated and not truncated:\n",
    "    action = ensemble.act({\"observation\": obs})\n",
    "    obs, reward, terminated, truncated, info = stock_trading_environment.step(action)\n",
    "    predictions.append(action)\n",
    "\n",
    "visualize_signals(stock_data.iloc[-len(predictions):], np.array(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9870ffa4",
   "metadata": {},
   "source": [
    "#### Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeca7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = stock_data['signal'].iloc[-len(predictions):].map({'buy': 0, 'sell': 1, 'none': 2}).values\n",
    "accuracy, precision, recall, f1 = calculate_metrics(y_true, predictions)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a311b7c3",
   "metadata": {},
   "source": [
    "##### Save Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba8d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(ensemble, 'aapl_ensemble_agent.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b416066",
   "metadata": {},
   "source": [
    "#### Forecast and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e23d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_ensemble = load_pickle(\"aapl_ensemble_agent.pkl\")\n",
    "\n",
    "stock_trading_environment = make_env('AAPL_data.csv', number_of_days_to_consider=30)\n",
    "stock_trading_environment.train = False\n",
    "obs, _ = stock_trading_environment.reset()\n",
    "terminated, truncated = False, False\n",
    "forecast_predictions = []\n",
    "while not terminated and not truncated:\n",
    "    action = loaded_ensemble.act({\"observation\": obs})\n",
    "    obs, reward, terminated, truncated, info = stock_trading_environment.step(action)\n",
    "    forecast_predictions.append(action)\n",
    "\n",
    "visualize_signals(stock_data.iloc[-len(forecast_predictions):], np.array(forecast_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e20a7e",
   "metadata": {},
   "source": [
    "#### Compare Original and Loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea34324",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_reward = run_learned_policy(stock_trading_environment, ensemble)\n",
    "loaded_reward = run_learned_policy(stock_trading_environment, loaded_ensemble)\n",
    "\n",
    "print(f\"Original model total reward: {original_reward}\")\n",
    "print(f\"Loaded model total reward: {loaded_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
