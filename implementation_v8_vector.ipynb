{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1cf939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from src.environment import StockTradingEnvironment, make_env\n",
    "from src.utils import save_pickle, load_pickle, plot_grid\n",
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "import yfinance as yf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    device = '/GPU:0'\n",
    "    logging.info(\"Using GPU for training\")\n",
    "else:\n",
    "    device = '/CPU:0'\n",
    "    logging.info(\"Using CPU for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169c51c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_stock_data(symbol, start_date, end_date, output_file):\n",
    "    stock_data = yf.download(symbol, start=start_date, end=end_date)\n",
    "    stock_data['Close'] = stock_data['Adj Close']\n",
    "    \n",
    "    stock_data = stock_data.drop(columns=['Adj Close'])\n",
    "\n",
    "    # Convert column headers to lowercase\n",
    "    stock_data.columns = stock_data.columns.str.lower()\n",
    "\n",
    "    stock_data.to_csv(output_file)\n",
    "        \n",
    "    return stock_data\n",
    "\n",
    "stock_data = fetch_stock_data('AAPL', '2019-06-20', '2024-06-21', 'AAPL_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42954e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss(y_true, y_pred, clip_delta=1.0):\n",
    "    error = y_true - y_pred\n",
    "    cond = K.abs(error) <= clip_delta\n",
    "    squared_loss = 0.5 * K.square(error)\n",
    "    quadratic_loss = 0.5 * K.square(clip_delta) + clip_delta * (K.abs(error) - clip_delta)\n",
    "    return K.mean(tf.where(cond, squared_loss, quadratic_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd4c31",
   "metadata": {},
   "source": [
    "#### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1cb59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.write = 0\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "        self.tree[parent] += change\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s - self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "        return idx, self.tree[idx], self.data[dataIdx]\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.alpha = alpha\n",
    "        self.capacity = capacity\n",
    "        self.epsilon = 0.01\n",
    "\n",
    "    def add(self, error, sample):\n",
    "        p = (error + self.epsilon) ** self.alpha\n",
    "        self.tree.add(p, sample)\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        batch = []\n",
    "        idxs = []\n",
    "        segment = self.tree.total() / batch_size\n",
    "        priorities = []\n",
    "        for i in range(batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            s = random.uniform(a, b)\n",
    "            idx, p, data = self.tree.get(s)\n",
    "            batch.append(data)\n",
    "            idxs.append(idx)\n",
    "            priorities.append(p)\n",
    "        sampling_probabilities = priorities / self.tree.total()\n",
    "        is_weights = np.power(self.tree.capacity * sampling_probabilities, -beta)\n",
    "        is_weights /= is_weights.max()\n",
    "        return batch, idxs, is_weights\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = (error + self.epsilon) ** self.alpha\n",
    "        self.tree.update(idx, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5f3ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleQLearningAgent:\n",
    "    def __init__(self, env, learning_rate, discount_factor, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995, bins=10):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.q_table1 = {}\n",
    "        self.q_table2 = {}\n",
    "        self.bins = bins\n",
    "\n",
    "        if isinstance(env.single_action_space, gym.spaces.Discrete):\n",
    "            self.action_space_type = 'Discrete'\n",
    "            self.action_space_size = env.single_action_space.n\n",
    "        elif isinstance(env.single_action_space, gym.spaces.MultiDiscrete):\n",
    "            self.action_space_type = 'MultiDiscrete'\n",
    "            self.action_space_size = env.single_action_space.nvec\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported action space type\")\n",
    "\n",
    "    def discretize_state(self, state):\n",
    "        discretized_state = tuple((state * self.bins).astype(int))\n",
    "        return discretized_state\n",
    "\n",
    "    def step(self, state):\n",
    "        observation = state[\"observation\"]\n",
    "        state_index = self.discretize_state(observation)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            if self.action_space_type == 'Discrete':\n",
    "                return np.random.randint(self.action_space_size)\n",
    "            elif self.action_space_type == 'MultiDiscrete':\n",
    "                return [np.random.randint(n) for n in self.action_space_size]\n",
    "        else:\n",
    "            q_values1 = self.q_table1.get(state_index, np.zeros(self.action_space_size))\n",
    "            q_values2 = self.q_table2.get(state_index, np.zeros(self.action_space_size))\n",
    "            q_values = q_values1 + q_values2\n",
    "            if self.action_space_type == 'Discrete':\n",
    "                return np.argmax(q_values)\n",
    "            elif self.action_space_type == 'MultiDiscrete':\n",
    "                return [np.argmax(q) for q in q_values]\n",
    "\n",
    "    def update_qvalue(self, state, action, reward, next_state):\n",
    "        observation = state[\"observation\"]\n",
    "        next_observation = next_state[\"observation\"]\n",
    "        state_index = self.discretize_state(observation)\n",
    "        next_state_index = self.discretize_state(next_observation)\n",
    "        if self.action_space_type == 'Discrete':\n",
    "            if np.random.rand() < 0.5:\n",
    "                best_next_action = np.argmax(self.q_table1.get(next_state_index, np.zeros(self.action_space_size)))\n",
    "                q_value_update = reward + self.discount_factor * self.q_table2.get(next_state_index, np.zeros(self.action_space_size))[best_next_action]\n",
    "                self.q_table1[state_index] = self.q_table1.get(state_index, np.zeros(self.action_space_size))\n",
    "                self.q_table1[state_index][action] += self.learning_rate * (q_value_update - self.q_table1[state_index][action])\n",
    "            else:\n",
    "                best_next_action = np.argmax(self.q_table2.get(next_state_index, np.zeros(self.action_space_size)))\n",
    "                q_value_update = reward + self.discount_factor * self.q_table1.get(next_state_index, np.zeros(self.action_space_size))[best_next_action]\n",
    "                self.q_table2[state_index] = self.q_table2.get(state_index, np.zeros(self.action_space_size))\n",
    "                self.q_table2[state_index][action] += self.learning_rate * (q_value_update - self.q_table2[state_index][action])\n",
    "        elif self.action_space_type == 'MultiDiscrete':\n",
    "            if np.random.rand() < 0.5:\n",
    "                best_next_action = [np.argmax(q) for q in self.q_table1.get(next_state_index, np.zeros(self.action_space_size))]\n",
    "                q_values2_next = self.q_table2.get(next_state_index, np.zeros(self.action_space_size))\n",
    "                update_value = reward + self.discount_factor * sum(q_values2_next[i, best_next_action[i]] for i in range(len(best_next_action)))\n",
    "                self.q_table1[state_index] = self.q_table1.get(state_index, np.zeros(self.action_space_size))\n",
    "                for i in range(len(action)):\n",
    "                    self.q_table1[state_index][i, action[i]] += self.learning_rate * (update_value - self.q_table1[state_index][i, action[i]])\n",
    "            else:\n",
    "                best_next_action = [np.argmax(q) for q in self.q_table2.get(next_state_index, np.zeros(self.action_space_size))]\n",
    "                q_values1_next = self.q_table1.get(next_state_index, np.zeros(self.action_space_size))\n",
    "                update_value = reward + self.discount_factor * sum(q_values1_next[i, best_next_action[i]] for i in range(len(best_next_action)))\n",
    "                self.q_table2[state_index] = self.q_table2.get(state_index, np.zeros(self.action_space_size))\n",
    "                for i in range(len(action)):\n",
    "                    self.q_table2[state_index][i, action[i]] += self.learning_rate * (update_value - self.q_table2[state_index][i, action[i]])\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, env, model_name='DQN_model', gamma=0.95, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995, learning_rate=0.001):\n",
    "        self.env = env\n",
    "        self.model_name = model_name\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = huber_loss\n",
    "        self.custom_objects = {\"huber_loss\": huber_loss}\n",
    "        self.optimizer = Adam(learning_rate=self.learning_rate)\n",
    "        self.buffer_size = 10000\n",
    "        self.batch_size = 32\n",
    "        self.memory = PrioritizedReplayBuffer(self.buffer_size, 0.6)\n",
    "        self.beta = 0.4\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=128, activation=\"relu\", input_dim=self.env.observation_space.shape[1]))\n",
    "        model.add(Dense(units=256, activation=\"relu\"))\n",
    "        model.add(Dense(units=256, activation=\"relu\"))\n",
    "        model.add(Dense(units=128, activation=\"relu\"))\n",
    "        model.add(Dense(units=self.env.action_space.n))\n",
    "        model.compile(loss=self.loss, optimizer=self.optimizer)\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        state = state.flatten().reshape(1, -1)\n",
    "        next_state = next_state.flatten().reshape(1, -1)\n",
    "        target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0]) if not done else reward\n",
    "        td_error = abs(target - np.amax(self.model.predict(state)[0]))\n",
    "        self.memory.add(td_error, (state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.env.action_space.n)\n",
    "        state = state.flatten().reshape(1, -1)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory.tree.data) < self.batch_size:\n",
    "            return\n",
    "        minibatch, idxs, is_weights = self.memory.sample(self.batch_size, self.beta)\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "        states = np.array(states).reshape(self.batch_size, -1)\n",
    "        next_states = np.array(next_states).reshape(self.batch_size, -1)\n",
    "        targets = self.model.predict(states)\n",
    "        targets_next = self.target_model.predict(next_states)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            target = rewards[i] + self.gamma * np.amax(targets_next[i]) if not dones[i] else rewards[i]\n",
    "            td_error = abs(target - targets[i][actions[i]])\n",
    "            self.memory.update(idxs[i], td_error)\n",
    "            targets[i][actions[i]] = target\n",
    "\n",
    "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
    "        self.update_target_model()\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22706b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_learning_loop(env, agent, learning_rate: float, discount_factor: float, episodes: int,\n",
    "                             min_epsilon_allowed: float, initial_epsilon_value: float,\n",
    "                             buffer_size: int = 10000, batch_size: int = 32, decay_method=\"exponential\") -> tuple:\n",
    "    epsilon = initial_epsilon_value\n",
    "    epsilon_decay_factor = np.power(min_epsilon_allowed / epsilon, 1 / episodes)\n",
    "\n",
    "    reward_across_episodes = []\n",
    "    epsilons_across_episodes = []\n",
    "\n",
    "    replay_buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        observations, _ = env.reset()\n",
    "        terminated = np.array([False] * env.num_envs)\n",
    "        truncated = np.array([False] * env.num_envs)\n",
    "        episode_rewards = np.zeros(env.num_envs)\n",
    "        \n",
    "        while not np.all(terminated) and not np.all(truncated):\n",
    "            actions = [agent.step({\"observation\": observation}) for observation in observations]\n",
    "            next_observations, rewards, terminated, truncated, _ = env.step(actions)\n",
    "            episode_rewards += rewards\n",
    "            \n",
    "            for obs, action, reward, next_obs, term, trun in zip(observations, actions, rewards, next_observations, terminated, truncated):\n",
    "                replay_buffer.append(({\"observation\": obs}, action, reward, {\"observation\": next_obs}, term or trun))\n",
    "                \n",
    "            observations = next_observations\n",
    "\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                batch = np.array(random.sample(replay_buffer, batch_size), dtype=object)\n",
    "                for b_state, b_action, b_reward, b_next_state, b_done in batch:\n",
    "                    b_state = agent.discretize_state(b_state[\"observation\"])\n",
    "                    b_next_state = agent.discretize_state(b_next_state[\"observation\"])\n",
    "                    if b_done:\n",
    "                        target = b_reward\n",
    "                    else:\n",
    "                        if isinstance(env.single_action_space, gym.spaces.Discrete):\n",
    "                            target = b_reward + discount_factor * np.max(agent.q_table1.get(b_next_state, np.zeros(env.single_action_space.n)) + agent.q_table2.get(b_next_state, np.zeros(env.single_action_space.n)))\n",
    "                        elif isinstance(env.single_action_space, gym.spaces.MultiDiscrete):\n",
    "                            target = b_reward + discount_factor * np.max(agent.q_table1.get(b_next_state, np.zeros(env.single_action_space.nvec)) + agent.q_table2.get(b_next_state, np.zeros(env.single_action_space.nvec)))\n",
    "                    \n",
    "                    if isinstance(env.single_action_space, gym.spaces.Discrete):\n",
    "                        agent.q_table1[b_state] = agent.q_table1.get(b_state, np.zeros(env.single_action_space.n))\n",
    "                    elif isinstance(env.single_action_space, gym.spaces.MultiDiscrete):\n",
    "                        agent.q_table1[b_state] = agent.q_table1.get(b_state, np.zeros(env.single_action_space.nvec))\n",
    "                    \n",
    "                    agent.q_table1[b_state][b_action] += learning_rate * (target - agent.q_table1[b_state][b_action])\n",
    "\n",
    "        if decay_method == \"exponential\":\n",
    "            epsilon = max(min_epsilon_allowed, epsilon * epsilon_decay_factor)\n",
    "        else:\n",
    "            epsilon = max(min_epsilon_allowed, epsilon - (initial_epsilon_value - min_epsilon_allowed) / episodes)\n",
    "\n",
    "        reward_across_episodes.append(np.mean(episode_rewards))\n",
    "        epsilons_across_episodes.append(epsilon)\n",
    "\n",
    "        agent.decay_epsilon()\n",
    "\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            logging.info(f\"Episode {episode + 1}/{episodes} - Mean Reward: {np.mean(episode_rewards)} - Epsilon: {epsilon}\")\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(reward_across_episodes, label='Rewards')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Mean Reward')\n",
    "    plt.title('Rewards over Episodes')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epsilons_across_episodes, label='Epsilon')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Epsilon')\n",
    "    plt.title('Epsilon Decay over Episodes')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return agent, reward_across_episodes, epsilons_across_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d38c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_envs = 3\n",
    "file_path = 'AAPL_data.csv'\n",
    "envs = gym.vector.SyncVectorEnv([lambda: make_env(file_path, number_of_days_to_consider=20, n_select=15) for _ in range(num_envs)])\n",
    "\n",
    "# Initialize Agent and Train\n",
    "agent = DoubleQLearningAgent(envs, learning_rate=0.001, discount_factor=0.95)\n",
    "\n",
    "agent, reward_across_episodes, epsilons_across_episodes = q_learning_learning_loop(\n",
    "    envs,\n",
    "    agent,\n",
    "    learning_rate=0.001,\n",
    "    discount_factor=0.95,\n",
    "    episodes=40000,\n",
    "    min_epsilon_allowed=0.01,\n",
    "    initial_epsilon_value=1,\n",
    "    buffer_size=10000,\n",
    "    batch_size=128,\n",
    "    decay_method=\"exponential\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed4414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_learned_policy(env, agent):\n",
    "    obs = env.reset()\n",
    "    terminated, truncated = np.array([False] * env.num_envs), np.array([False] * env.num_envs)\n",
    "    total_rewards = np.zeros(env.num_envs)\n",
    "\n",
    "    while not np.all(terminated) and not np.all(truncated):\n",
    "        actions = [np.argmax(agent.q_table1.get(tuple(observation.flatten()), np.zeros(env.action_space.n)) + agent.q_table2.get(tuple(observation.flatten()), np.zeros(env.action_space.n))) for observation in obs]\n",
    "        obs, rewards, terminated, truncated, _ = env.step(actions)\n",
    "        total_rewards += rewards\n",
    "\n",
    "    logging.info(\"Total Rewards: %s\", total_rewards)\n",
    "    return total_rewards\n",
    "\n",
    "total_rewards = run_learned_policy(envs, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5821cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid(env, agent, reward_across_episodes: list, epsilons_across_episodes: list) -> None:\n",
    "    env.train = False\n",
    "    total_reward_learned_policy = [run_learned_policy(env, agent) for _ in range(30)]\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(reward_across_episodes, 'ro')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward Value')\n",
    "    plt.title('Rewards Per Episode (Training)')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(total_reward_learned_policy, 'ro')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward Value')\n",
    "    plt.title('Rewards Per Episode (Learned Policy Evaluation)')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(reward_across_episodes)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulative Reward Per Episode (Training)')\n",
    "    plt.title('Cumulative Reward vs Episode')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(epsilons_across_episodes)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Epsilon Values')\n",
    "    plt.title('Epsilon Decay')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_grid(envs, agent, reward_across_episodes, epsilons_across_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25de7bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_trading_environment = StockTradingEnvironment('./AAPL_data.csv', number_of_days_to_consider=30)\n",
    "stock_trading_environment.train = False\n",
    "obs, _ = stock_trading_environment.reset()\n",
    "terminated, truncated = False, False\n",
    "while not terminated:\n",
    "    # Convert observation to a tuple\n",
    "    obs_tuple = tuple(obs.flatten())\n",
    "    action = np.argmax(agent.q_table1.get(obs_tuple, np.zeros(envs.action_space.n)) + agent.q_table2.get(obs_tuple, np.zeros(envs.action_space.n)))\n",
    "    obs, reward, terminated, truncated, info = stock_trading_environment.step(action)\n",
    "\n",
    "stock_trading_environment.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba8d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(agent, 'aapl_q_learning_agent.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b416066",
   "metadata": {},
   "source": [
    "#### Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2ec473",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = load_pickle(\"aapl_q_learning_agent.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e23d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_trading_environment = make_env('AAPL_data.csv', number_of_days_to_consider=30)\n",
    "stock_trading_environment.train = False\n",
    "obs, _ = stock_trading_environment.reset()\n",
    "terminated, truncated = False, False\n",
    "while not terminated:\n",
    "    obs_tuple = tuple(obs.flatten())\n",
    "    action = np.argmax(agent.q_table1.get(obs_tuple, np.zeros(envs.action_space.n)) + agent.q_table2.get(obs_tuple, np.zeros(envs.action_space.n)))\n",
    "    obs, reward, terminated, truncated, info = stock_trading_environment.step(action)\n",
    "\n",
    "stock_trading_environment.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
