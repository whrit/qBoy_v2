{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f1cf939",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 15:59:45,363 - INFO - Using GPU for training\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from src.environmentv1 import StockTradingEnvironment\n",
    "from src.utils import save_pickle, load_pickle, plot_grid\n",
    "from src.learner import q_learning_learning_loop\n",
    "import yfinance as yf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    device = '/GPU:0'\n",
    "    logging.info(\"Using GPU for training\")\n",
    "else:\n",
    "    device = '/CPU:0'\n",
    "    logging.info(\"Using CPU for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169c51c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yfinance download data\n",
    "def fetch_stock_data(symbol, start_date, end_date, output_file):\n",
    "    stock_data = yf.download(symbol, start=start_date, end=end_date)\n",
    "    stock_data['Close'] = stock_data['Adj Close']\n",
    "    \n",
    "    stock_data = stock_data.drop(columns=['Adj Close'])\n",
    "\n",
    "    stock_data.to_csv(output_file)\n",
    "        \n",
    "    return stock_data\n",
    "\n",
    "stock_data = fetch_stock_data('AAPL', '2019-06-20', '2024-06-21', 'AAPL_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "545c5bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss(y_true, y_pred, clip_delta=1.0):\n",
    "    \"\"\"Huber loss - Custom Loss Function for Q Learning\n",
    "\n",
    "    Links: \thttps://en.wikipedia.org/wiki/Huber_loss\n",
    "            https://jaromiru.com/2017/05/27/on-using-huber-loss-in-deep-q-learning/\n",
    "    \"\"\"\n",
    "    error = y_true - y_pred\n",
    "    cond = K.abs(error) <= clip_delta\n",
    "    squared_loss = 0.5 * K.square(error)\n",
    "    quadratic_loss = 0.5 * K.square(clip_delta) + clip_delta * (K.abs(error) - clip_delta)\n",
    "    return K.mean(tf.where(cond, squared_loss, quadratic_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd4c31",
   "metadata": {},
   "source": [
    "#### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b1cb59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.write = 0\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "        self.tree[parent] += change\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s - self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "        return idx, self.tree[idx], self.data[dataIdx]\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.alpha = alpha\n",
    "        self.capacity = capacity\n",
    "        self.epsilon = 0.01\n",
    "\n",
    "    def add(self, error, sample):\n",
    "        p = (error + self.epsilon) ** self.alpha\n",
    "        self.tree.add(p, sample)\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        batch = []\n",
    "        idxs = []\n",
    "        segment = self.tree.total() / batch_size\n",
    "        priorities = []\n",
    "        for i in range(batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            s = random.uniform(a, b)\n",
    "            idx, p, data = self.tree.get(s)\n",
    "            batch.append(data)\n",
    "            idxs.append(idx)\n",
    "            priorities.append(p)\n",
    "        sampling_probabilities = priorities / self.tree.total()\n",
    "        is_weights = np.power(self.tree.capacity * sampling_probabilities, -beta)\n",
    "        is_weights /= is_weights.max()\n",
    "        return batch, idxs, is_weights\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = (error + self.epsilon) ** self.alpha\n",
    "        self.tree.update(idx, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e5f3ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleQLearningAgent:\n",
    "    def __init__(self, env, learning_rate, discount_factor):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.q_table1 = {}\n",
    "        self.q_table2 = {}\n",
    "\n",
    "    def step(self, state, epsilon):\n",
    "        state_index = self._get_state_index(state)\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.choice(self.env.action_space.n)\n",
    "        else:\n",
    "            q_values = self.q_table1.get(state_index, np.zeros(self.env.action_space.n)) + self.q_table2.get(state_index, np.zeros(self.env.action_space.n))\n",
    "            return np.argmax(q_values)\n",
    "\n",
    "    def update_qvalue(self, state, action, reward, next_state):\n",
    "        state_index = self._get_state_index(state)\n",
    "        next_state_index = self._get_state_index(next_state)\n",
    "        if np.random.rand() < 0.5:\n",
    "            best_next_action = np.argmax(self.q_table1.get(next_state_index, np.zeros(self.env.action_space.n)))\n",
    "            self.q_table1[state_index] = self.q_table1.get(state_index, np.zeros(self.env.action_space.n))\n",
    "            self.q_table1[state_index][action] += self.learning_rate * (\n",
    "                reward + self.discount_factor * self.q_table2.get(next_state_index, np.zeros(self.env.action_space.n))[best_next_action] - self.q_table1[state_index][action]\n",
    "            )\n",
    "        else:\n",
    "            best_next_action = np.argmax(self.q_table2.get(next_state_index, np.zeros(self.env.action_space.n)))\n",
    "            self.q_table2[state_index] = self.q_table2.get(state_index, np.zeros(self.env.action_space.n))\n",
    "            self.q_table2[state_index][action] += self.learning_rate * (\n",
    "                reward + self.discount_factor * self.q_table1.get(next_state_index, np.zeros(self.env.action_space.n))[best_next_action] - self.q_table2[state_index][action]\n",
    "            )\n",
    "\n",
    "    def _get_state_index(self, state):\n",
    "        return tuple(state.flatten())\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env, model_name='DQN_model', gamma=0.95, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995, learning_rate=0.001):\n",
    "        self.env = env\n",
    "        self.model_name = model_name\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = self.huber_loss\n",
    "        self.custom_objects = {\"huber_loss\": self.huber_loss}\n",
    "        self.optimizer = Adam(lr=self.learning_rate)\n",
    "        self.buffer_size = 10000\n",
    "        self.batch_size = 32\n",
    "        self.memory = PrioritizedReplayBuffer(self.buffer_size, 0.6)\n",
    "        self.beta = 0.4\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=128, activation=\"relu\", input_dim=self.env.observation_space.shape[0]))\n",
    "        model.add(Dense(units=256, activation=\"relu\"))\n",
    "        model.add(Dense(units=256, activation=\"relu\"))\n",
    "        model.add(Dense(units=128, activation=\"relu\"))\n",
    "        model.add(Dense(units=self.env.action_space.n))\n",
    "        model.compile(loss=self.loss, optimizer=self.optimizer)\n",
    "        return model\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        state = state.flatten().reshape(1, -1)\n",
    "        next_state = next_state.flatten().reshape(1, -1)\n",
    "        target = reward + self.discount_factor * np.amax(self.target_model.predict(next_state)[0]) if not done else reward\n",
    "        td_error = abs(target - np.amax(self.model.predict(state)[0]))\n",
    "        self.memory.add(td_error, (state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        if np.random.rand() <= epsilon:\n",
    "            return np.random.choice(self.env.action_space.n)\n",
    "        state = state.flatten().reshape(1, -1)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory.tree.data) < self.batch_size:\n",
    "            return\n",
    "        minibatch, idxs, is_weights = self.memory.sample(self.batch_size, self.beta)\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "        states = np.array(states).reshape(self.batch_size, -1)\n",
    "        next_states = np.array(next_states).reshape(self.batch_size, -1)\n",
    "        targets = self.model.predict(states)\n",
    "        targets_next = self.target_model.predict(next_states)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            target = rewards[i] + self.discount_factor * np.amax(targets_next[i]) if not dones[i] else rewards[i]\n",
    "            td_error = abs(target - targets[i][actions[i]])\n",
    "            self.memory.update(idxs[i], td_error)\n",
    "            targets[i][actions[i]] = target\n",
    "\n",
    "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
    "        self.update_target_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22706b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_experience(env, agent, epsilon):\n",
    "    obs, _ = env.reset()\n",
    "    terminated, truncated = False, False\n",
    "    experiences = []\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        current_action = agent.step(obs, epsilon)\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(current_action)\n",
    "        experiences.append((obs, current_action, reward, next_obs, terminated))\n",
    "        obs = next_obs\n",
    "    \n",
    "    return experiences\n",
    "\n",
    "def adaptive_learning_rate(epoch, lr):\n",
    "    if epoch % 100 == 0 and epoch:\n",
    "        return lr * 0.1\n",
    "    return lr\n",
    "\n",
    "def decay_epsilon(episode, epsilon, min_epsilon, decay_rate):\n",
    "    return max(min_epsilon, epsilon * decay_rate)\n",
    "\n",
    "def q_learning_learning_loop(env, agent, learning_rate: float, discount_factor: float, episodes: int,\n",
    "                             min_epsilon_allowed: float, initial_epsilon_value: float,\n",
    "                             buffer_size: int = 10000, batch_size: int = 32, decay_method=\"exponential\") -> tuple:\n",
    "    epsilon = initial_epsilon_value\n",
    "    epsilon_decay_factor = np.power(min_epsilon_allowed / epsilon, 1 / episodes)\n",
    "\n",
    "    reward_across_episodes = []\n",
    "    epsilons_across_episodes = []\n",
    "\n",
    "    replay_buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        experiences = collect_experience(env, agent, epsilon)\n",
    "        replay_buffer.extend(experiences)\n",
    "\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            batch = np.array(random.sample(replay_buffer, batch_size), dtype=object)\n",
    "            for b_state, b_action, b_reward, b_next_state, b_done in batch:\n",
    "                b_state = tuple(b_state.flatten())  # Convert to tuple\n",
    "                b_next_state = tuple(b_next_state.flatten())  # Convert to tuple\n",
    "                if b_done:\n",
    "                    target = b_reward\n",
    "                else:\n",
    "                    target = b_reward + discount_factor * np.max(agent.q_table1.get(b_next_state, np.zeros(env.action_space.n)) + agent.q_table2.get(b_next_state, np.zeros(env.action_space.n)))\n",
    "\n",
    "                agent.q_table1[b_state] = agent.q_table1.get(b_state, np.zeros(env.action_space.n))\n",
    "                agent.q_table1[b_state][b_action] += learning_rate * (target - agent.q_table1[b_state][b_action])\n",
    "\n",
    "        if decay_method == \"exponential\":\n",
    "            epsilon = decay_epsilon(episode, epsilon, min_epsilon_allowed, epsilon_decay_factor)\n",
    "        else:\n",
    "            epsilon = max(min_epsilon_allowed, epsilon - (initial_epsilon_value - min_epsilon_allowed) / episodes)\n",
    "        \n",
    "        reward_across_episodes.append(sum([exp[2] for exp in experiences]))\n",
    "        epsilons_across_episodes.append(epsilon)\n",
    "\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            logging.info(f\"Episode {episode + 1}/{episodes} - Reward: {reward_across_episodes[-1]} - Epsilon: {epsilon}\")\n",
    "\n",
    "    logging.info(\"Trained Q-Table: %s\", agent.q_table1)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(reward_across_episodes, label='Rewards')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.title('Rewards over Episodes')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epsilons_across_episodes, label='Epsilon')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Epsilon')\n",
    "    plt.title('Epsilon Decay over Episodes')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return agent, reward_across_episodes, epsilons_across_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d70e1189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_collect_experience(env, agent, epsilon, num_workers=8):\n",
    "    with mp.Pool(num_workers) as pool:\n",
    "        results = pool.starmap(collect_experience, [(env, agent, epsilon) for _ in range(num_workers)])\n",
    "    experiences = [exp for result in results for exp in result]\n",
    "    return experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca06c6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: You can adjust the parameter 'number_of_days_to_consider'\n",
    "\n",
    "# env = stock_trading_environment = StockTradingEnvironment('./AAPL_data.csv', number_of_days_to_consider=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4d38c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 16:02:02,938 - INFO - Episode 100/10000 - Reward: -3441.3263795370012 - Epsilon: 0.9549925860214323\n",
      "2024-06-24 16:04:04,613 - INFO - Episode 200/10000 - Reward: -3351.1389670451513 - Epsilon: 0.9120108393559037\n",
      "2024-06-24 16:06:08,937 - INFO - Episode 300/10000 - Reward: -3600.5355282537694 - Epsilon: 0.870963589956072\n",
      "2024-06-24 16:08:13,009 - INFO - Episode 400/10000 - Reward: -3720.463004170636 - Epsilon: 0.8317637711026604\n",
      "2024-06-24 16:10:18,368 - INFO - Episode 500/10000 - Reward: -3578.9802829742007 - Epsilon: 0.7943282347242693\n",
      "2024-06-24 16:12:22,274 - INFO - Episode 600/10000 - Reward: -4036.4516692349766 - Epsilon: 0.7585775750291692\n",
      "2024-06-24 16:14:24,961 - INFO - Episode 700/10000 - Reward: -3794.2332300287767 - Epsilon: 0.7244359600749737\n",
      "2024-06-24 16:16:27,942 - INFO - Episode 800/10000 - Reward: -4110.1401374193065 - Epsilon: 0.691830970918919\n",
      "2024-06-24 16:19:11,618 - INFO - Episode 900/10000 - Reward: -4501.117061400562 - Epsilon: 0.6606934480075775\n",
      "2024-06-24 16:21:50,556 - INFO - Episode 1000/10000 - Reward: -4383.635924745003 - Epsilon: 0.6309573444801734\n",
      "2024-06-24 16:24:43,476 - INFO - Episode 1100/10000 - Reward: -4861.74683426304 - Epsilon: 0.6025595860743366\n",
      "2024-06-24 16:26:52,286 - INFO - Episode 1200/10000 - Reward: -4725.953032444467 - Epsilon: 0.5754399373371345\n",
      "2024-06-24 16:29:02,437 - INFO - Episode 1300/10000 - Reward: -4619.332779681118 - Epsilon: 0.5495408738576012\n",
      "2024-06-24 16:31:11,363 - INFO - Episode 1400/10000 - Reward: -4982.175169193884 - Epsilon: 0.5248074602497483\n",
      "2024-06-24 16:33:24,145 - INFO - Episode 1500/10000 - Reward: -5003.345906730931 - Epsilon: 0.5011872336272477\n",
      "2024-06-24 16:35:36,194 - INFO - Episode 1600/10000 - Reward: -5015.786061100636 - Epsilon: 0.47863009232261333\n",
      "2024-06-24 16:38:36,178 - INFO - Episode 1700/10000 - Reward: -5850.597760931107 - Epsilon: 0.4570881896148494\n",
      "2024-06-24 16:41:51,878 - INFO - Episode 1800/10000 - Reward: -5578.77789005202 - Epsilon: 0.43651583224014007\n",
      "2024-06-24 16:47:17,031 - INFO - Episode 1900/10000 - Reward: -5798.3116293521625 - Epsilon: 0.4168693834703095\n",
      "2024-06-24 16:51:38,967 - INFO - Episode 2000/10000 - Reward: -5865.142975242568 - Epsilon: 0.39810717055347095\n",
      "2024-06-24 16:55:45,040 - INFO - Episode 2100/10000 - Reward: -6077.206645137458 - Epsilon: 0.3801893963205349\n",
      "2024-06-24 17:00:30,228 - INFO - Episode 2200/10000 - Reward: -5831.47766851903 - Epsilon: 0.3630780547700753\n",
      "2024-06-24 17:14:05,655 - INFO - Episode 2300/10000 - Reward: -6288.88578867003 - Epsilon: 0.3467368504525059\n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x103183730>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/beckett/miniconda3/envs/tradeEnv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "env = stock_trading_environment = StockTradingEnvironment('./AAPL_data.csv', number_of_days_to_consider=20)\n",
    "\n",
    "agent = DoubleQLearningAgent(env, learning_rate=0.001, discount_factor=0.95)\n",
    "\n",
    "agent, reward_across_episodes, epsilons_across_episodes = q_learning_learning_loop(\n",
    "    env,\n",
    "    agent,\n",
    "    learning_rate=0.001,\n",
    "    discount_factor=0.95,\n",
    "    episodes=10000,\n",
    "    min_epsilon_allowed=0.01,\n",
    "    initial_epsilon_value=1,\n",
    "    buffer_size=100000,\n",
    "    batch_size=128,\n",
    "    decay_method=\"exponential\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed4414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_learned_policy(env, agent):\n",
    "    obs, _ = env.reset()\n",
    "    terminated, truncated = False, False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        obs_tuple = tuple(obs.flatten())\n",
    "        action = np.argmax(agent.q_table1.get(obs_tuple, np.zeros(env.action_space.n)) + agent.q_table2.get(obs_tuple, np.zeros(env.action_space.n)))\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "    logging.info(\"Total Reward: %d, Steps: %d\", total_reward, steps)\n",
    "    return total_reward\n",
    "\n",
    "total_reward = run_learned_policy(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5821cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid(env, agent, reward_across_episodes: list, epsilons_across_episodes: list) -> None:\n",
    "    env.train = False\n",
    "    total_reward_learned_policy = [run_learned_policy(env, agent) for _ in range(30)]\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Main plot\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(reward_across_episodes, 'ro')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward Value')\n",
    "    plt.title('Rewards Per Episode (Training)')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(total_reward_learned_policy, 'ro')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward Value')\n",
    "    plt.title('Rewards Per Episode (Learned Policy Evaluation)')\n",
    "    plt.grid()\n",
    "\n",
    "    # Extra plots\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(reward_across_episodes)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulative Reward Per Episode (Training)')\n",
    "    plt.title('Cumulative Reward vs Episode')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(epsilons_across_episodes)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Epsilon Values')\n",
    "    plt.title('Epsilon Decay')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_grid(env, agent, reward_across_episodes, epsilons_across_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a3c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_trading_environment = StockTradingEnvironment('./AAPL_data.csv', number_of_days_to_consider=30)\n",
    "stock_trading_environment.train = False\n",
    "obs, _ = stock_trading_environment.reset()\n",
    "terminated, truncated = False, False\n",
    "while not terminated:\n",
    "    # Convert observation to a tuple\n",
    "    obs_tuple = tuple(obs.flatten())\n",
    "    action = np.argmax(agent.q_table1.get(obs_tuple, np.zeros(env.action_space.n)) + agent.q_table2.get(obs_tuple, np.zeros(env.action_space.n)))\n",
    "    obs, reward, terminated, truncated, info = stock_trading_environment.step(action)\n",
    "\n",
    "stock_trading_environment.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba8d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(agent, 'aapl_q_learning_agent.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b416066",
   "metadata": {},
   "source": [
    "#### Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2ec473",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = load_pickle(\"aapl_q_learning_agent.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e23d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_trading_environment = StockTradingEnvironment('./AAPL_data.csv', number_of_days_to_consider=30)\n",
    "stock_trading_environment.train = False\n",
    "obs, _ = stock_trading_environment.reset()\n",
    "terminated, truncated = False, False\n",
    "while not terminated:\n",
    "    # Convert observation to a tuple\n",
    "    obs_tuple = tuple(obs.flatten())\n",
    "    action = np.argmax(agent.q_table1.get(obs_tuple, np.zeros(env.action_space.n)) + agent.q_table2.get(obs_tuple, np.zeros(env.action_space.n)))\n",
    "    obs, reward, terminated, truncated, info = stock_trading_environment.step(action)\n",
    "\n",
    "stock_trading_environment.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
