{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f1cf939",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 14:33:49,569 - INFO - Using GPU for training\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from src.environment import StockTradingEnvironment\n",
    "from src.utils import save_pickle, load_pickle, plot_grid\n",
    "from src.learner import q_learning_learning_loop\n",
    "import yfinance as yf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    device = '/GPU:0'\n",
    "    logging.info(\"Using GPU for training\")\n",
    "else:\n",
    "    device = '/CPU:0'\n",
    "    logging.info(\"Using CPU for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169c51c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_stock_data(symbol, start_date, end_date, output_file):\n",
    "    stock_data = yf.download(symbol, start=start_date, end=end_date)\n",
    "    stock_data['Close'] = stock_data['Adj Close']\n",
    "    \n",
    "    stock_data = stock_data.drop(columns=['Adj Close'])\n",
    "\n",
    "    # Convert column headers to lowercase\n",
    "    stock_data.columns = stock_data.columns.str.lower()\n",
    "\n",
    "    stock_data.to_csv(output_file)\n",
    "        \n",
    "    return stock_data\n",
    "\n",
    "stock_data = fetch_stock_data('AAPL', '2019-06-20', '2024-06-21', 'AAPL_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a42954e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss(y_true, y_pred, clip_delta=1.0):\n",
    "    \"\"\"Huber loss - Custom Loss Function for Q Learning\n",
    "\n",
    "    Links: \thttps://en.wikipedia.org/wiki/Huber_loss\n",
    "            https://jaromiru.com/2017/05/27/on-using-huber-loss-in-deep-q-learning/\n",
    "    \"\"\"\n",
    "    error = y_true - y_pred\n",
    "    cond = K.abs(error) <= clip_delta\n",
    "    squared_loss = 0.5 * K.square(error)\n",
    "    quadratic_loss = 0.5 * K.square(clip_delta) + clip_delta * (K.abs(error) - clip_delta)\n",
    "    return K.mean(tf.where(cond, squared_loss, quadratic_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd4c31",
   "metadata": {},
   "source": [
    "#### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b1cb59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.write = 0\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "        self.tree[parent] += change\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s - self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "        return idx, self.tree[idx], self.data[dataIdx]\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.alpha = alpha\n",
    "        self.capacity = capacity\n",
    "        self.epsilon = 0.01\n",
    "\n",
    "    def add(self, error, sample):\n",
    "        p = (error + self.epsilon) ** self.alpha\n",
    "        self.tree.add(p, sample)\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        batch = []\n",
    "        idxs = []\n",
    "        segment = self.tree.total() / batch_size\n",
    "        priorities = []\n",
    "        for i in range(batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            s = random.uniform(a, b)\n",
    "            idx, p, data = self.tree.get(s)\n",
    "            batch.append(data)\n",
    "            idxs.append(idx)\n",
    "            priorities.append(p)\n",
    "        sampling_probabilities = priorities / self.tree.total()\n",
    "        is_weights = np.power(self.tree.capacity * sampling_probabilities, -beta)\n",
    "        is_weights /= is_weights.max()\n",
    "        return batch, idxs, is_weights\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = (error + self.epsilon) ** self.alpha\n",
    "        self.tree.update(idx, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e5f3ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleQLearningAgent:\n",
    "    def __init__(self, env, learning_rate, discount_factor, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.q_table1 = {}\n",
    "        self.q_table2 = {}\n",
    "\n",
    "    def step(self, state):\n",
    "        state_index = self._get_state_index(state)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.env.action_space.n)\n",
    "        else:\n",
    "            q_values = self.q_table1.get(state_index, np.zeros(self.env.action_space.n)) + self.q_table2.get(state_index, np.zeros(self.env.action_space.n))\n",
    "            return np.argmax(q_values)\n",
    "\n",
    "    def update_qvalue(self, state, action, reward, next_state):\n",
    "        state_index = self._get_state_index(state)\n",
    "        next_state_index = self._get_state_index(next_state)\n",
    "        if np.random.rand() < 0.5:\n",
    "            best_next_action = np.argmax(self.q_table1.get(next_state_index, np.zeros(self.env.action_space.n)))\n",
    "            self.q_table1[state_index] = self.q_table1.get(state_index, np.zeros(self.env.action_space.n))\n",
    "            self.q_table1[state_index][action] += self.learning_rate * (\n",
    "                reward + self.discount_factor * self.q_table2.get(next_state_index, np.zeros(self.env.action_space.n))[best_next_action] - self.q_table1[state_index][action]\n",
    "            )\n",
    "        else:\n",
    "            best_next_action = np.argmax(self.q_table2.get(next_state_index, np.zeros(self.env.action_space.n)))\n",
    "            self.q_table2[state_index] = self.q_table2.get(state_index, np.zeros(self.env.action_space.n))\n",
    "            self.q_table2[state_index][action] += self.learning_rate * (\n",
    "                reward + self.discount_factor * self.q_table1.get(next_state_index, np.zeros(self.env.action_space.n))[best_next_action] - self.q_table2[state_index][action]\n",
    "            )\n",
    "\n",
    "    def _get_state_index(self, state):\n",
    "        return tuple(state.flatten())\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env, model_name='DQN_model', gamma=0.95, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995, learning_rate=0.001):\n",
    "        self.env = env\n",
    "        self.model_name = model_name\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = self.huber_loss\n",
    "        self.custom_objects = {\"huber_loss\": self.huber_loss}\n",
    "        self.optimizer = Adam(lr=self.learning_rate)\n",
    "        self.buffer_size = 10000\n",
    "        self.batch_size = 32\n",
    "        self.memory = PrioritizedReplayBuffer(self.buffer_size, 0.6)\n",
    "        self.beta = 0.4\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=128, activation=\"relu\", input_dim=self.env.observation_space.shape[0]))\n",
    "        model.add(Dense(units=256, activation=\"relu\"))\n",
    "        model.add(Dense(units=256, activation=\"relu\"))\n",
    "        model.add(Dense(units=128, activation=\"relu\"))\n",
    "        model.add(Dense(units=self.env.action_space.n))\n",
    "        model.compile(loss=self.loss, optimizer=self.optimizer)\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        state = state.flatten().reshape(1, -1)\n",
    "        next_state = next_state.flatten().reshape(1, -1)\n",
    "        target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0]) if not done else reward\n",
    "        td_error = abs(target - np.amax(self.model.predict(state)[0]))\n",
    "        self.memory.add(td_error, (state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.env.action_space.n)\n",
    "        state = state.flatten().reshape(1, -1)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory.tree.data) < self.batch_size:\n",
    "            return\n",
    "        minibatch, idxs, is_weights = self.memory.sample(self.batch_size, self.beta)\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "        states = np.array(states).reshape(self.batch_size, -1)\n",
    "        next_states = np.array(next_states).reshape(self.batch_size, -1)\n",
    "        targets = self.model.predict(states)\n",
    "        targets_next = self.target_model.predict(next_states)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            target = rewards[i] + self.gamma * np.amax(targets_next[i]) if not dones[i] else rewards[i]\n",
    "            td_error = abs(target - targets[i][actions[i]])\n",
    "            self.memory.update(idxs[i], td_error)\n",
    "            targets[i][actions[i]] = target\n",
    "\n",
    "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
    "        self.update_target_model()\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22706b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_experience(env, agent, epsilon):\n",
    "    obs, _ = env.reset()\n",
    "    terminated, truncated = False, False\n",
    "    experiences = []\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        current_action = agent.step(obs)\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(current_action)\n",
    "        experiences.append((obs, current_action, reward, next_obs, terminated))\n",
    "        obs = next_obs\n",
    "\n",
    "    return experiences\n",
    "\n",
    "def adaptive_learning_rate(epoch, lr):\n",
    "    if epoch % 100 == 0 and epoch:\n",
    "        return lr * 0.1\n",
    "    return lr\n",
    "\n",
    "def decay_epsilon(episode, epsilon, min_epsilon, decay_rate):\n",
    "    return max(min_epsilon, epsilon * decay_rate)\n",
    "\n",
    "def q_learning_learning_loop(env, agent, learning_rate: float, discount_factor: float, episodes: int,\n",
    "                             min_epsilon_allowed: float, initial_epsilon_value: float,\n",
    "                             buffer_size: int = 10000, batch_size: int = 32, decay_method=\"exponential\") -> tuple:\n",
    "    epsilon = initial_epsilon_value\n",
    "    epsilon_decay_factor = np.power(min_epsilon_allowed / epsilon, 1 / episodes)\n",
    "\n",
    "    reward_across_episodes = []\n",
    "    epsilons_across_episodes = []\n",
    "\n",
    "    replay_buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        experiences = collect_experience(env, agent, epsilon)\n",
    "        replay_buffer.extend(experiences)\n",
    "\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            batch = np.array(random.sample(replay_buffer, batch_size), dtype=object)\n",
    "            for b_state, b_action, b_reward, b_next_state, b_done in batch:\n",
    "                b_state = tuple(b_state.flatten())\n",
    "                b_next_state = tuple(b_next_state.flatten())\n",
    "                if b_done:\n",
    "                    target = b_reward\n",
    "                else:\n",
    "                    target = b_reward + discount_factor * np.max(agent.q_table1.get(b_next_state, np.zeros(env.action_space.n)) + agent.q_table2.get(b_next_state, np.zeros(env.action_space.n)))\n",
    "\n",
    "                agent.q_table1[b_state] = agent.q_table1.get(b_state, np.zeros(env.action_space.n))\n",
    "                agent.q_table1[b_state][b_action] += learning_rate * (target - agent.q_table1[b_state][b_action])\n",
    "\n",
    "        if decay_method == \"exponential\":\n",
    "            epsilon = decay_epsilon(episode, epsilon, min_epsilon_allowed, epsilon_decay_factor)\n",
    "        else:\n",
    "            epsilon = max(min_epsilon_allowed, epsilon - (initial_epsilon_value - min_epsilon_allowed) / episodes)\n",
    "\n",
    "        reward_across_episodes.append(sum([exp[2] for exp in experiences]))\n",
    "        epsilons_across_episodes.append(epsilon)\n",
    "\n",
    "        agent.decay_epsilon()  # Apply epsilon decay after each episode\n",
    "\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            logging.info(f\"Episode {episode + 1}/{episodes} - Reward: {reward_across_episodes[-1]} - Epsilon: {epsilon}\")\n",
    "\n",
    "    logging.info(\"Trained Q-Table: %s\", agent.q_table1)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(reward_across_episodes, label='Rewards')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.title('Rewards over Episodes')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epsilons_across_episodes, label='Epsilon')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Epsilon')\n",
    "    plt.title('Epsilon Decay over Episodes')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return agent, reward_across_episodes, epsilons_across_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d70e1189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_collect_experience(env, agent, epsilon, num_workers=8):\n",
    "    with mp.Pool(num_workers) as pool:\n",
    "        results = pool.starmap(collect_experience, [(env, agent, epsilon) for _ in range(num_workers)])\n",
    "    experiences = [exp for result in results for exp in result]\n",
    "    return experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca06c6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: You can adjust the parameter 'number_of_days_to_consider'\n",
    "\n",
    "# env = stock_trading_environment = StockTradingEnvironment('./AAPL_data.csv', number_of_days_to_consider=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4d38c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def train_agent(trial):\n",
    "    # Suggest parameters\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    episodes = trial.suggest_categorical('episodes', [1000, 5000, 10000, 50000])\n",
    "    days_to_consider = trial.suggest_categorical('days_to_consider', [10, 20, 30, 40])\n",
    "    buffer_size = trial.suggest_categorical('buffer_size', [10000, 50000, 100000, 200000])\n",
    "    \n",
    "    # Initialize the environment and agent with the current parameters\n",
    "    env = StockTradingEnvironment('./AAPL_data.csv', number_of_days_to_consider=days_to_consider)\n",
    "    agent = DoubleQLearningAgent(env, learning_rate=0.001, discount_factor=0.95)\n",
    "    \n",
    "    # Train the agent\n",
    "    agent, reward_across_episodes, epsilons_across_episodes = q_learning_learning_loop(\n",
    "        env,\n",
    "        agent,\n",
    "        learning_rate=0.001,\n",
    "        discount_factor=0.95,\n",
    "        episodes=episodes,\n",
    "        min_epsilon_allowed=0.01,\n",
    "        initial_epsilon_value=1,\n",
    "        buffer_size=buffer_size,\n",
    "        batch_size=batch_size,\n",
    "        decay_method=\"exponential\"\n",
    "    )\n",
    "    \n",
    "    # Evaluate the agent\n",
    "    total_reward = sum(reward_across_episodes)\n",
    "    return total_reward\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(train_agent, n_trials=50)\n",
    "\n",
    "print(f\"Best Parameters: {study.best_params}\")\n",
    "print(f\"Best Score: {study.best_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bed4414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_learned_policy(env, agent):\n",
    "    obs, _ = env.reset()\n",
    "    terminated, truncated = False, False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        obs_tuple = tuple(obs.flatten())\n",
    "        action = np.argmax(agent.q_table1.get(obs_tuple, np.zeros(env.action_space.n)) + agent.q_table2.get(obs_tuple, np.zeros(env.action_space.n)))\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "    logging.info(\"Total Reward: %d, Steps: %d\", total_reward, steps)\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3e35d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "env = StockTradingEnvironment('./AAPL_data.csv', number_of_days_to_consider=best_params['days_to_consider'])\n",
    "agent = DoubleQLearningAgent(env, learning_rate=0.001, discount_factor=0.95)\n",
    "\n",
    "# Train the agent with the best parameters\n",
    "agent, reward_across_episodes, epsilons_across_episodes = q_learning_learning_loop(\n",
    "    env,\n",
    "    agent,\n",
    "    learning_rate=0.001,\n",
    "    discount_factor=0.95,\n",
    "    episodes=best_params['episodes'],\n",
    "    min_epsilon_allowed=0.01,\n",
    "    initial_epsilon_value=1,\n",
    "    buffer_size=best_params['buffer_size'],\n",
    "    batch_size=best_params['batch_size'],\n",
    "    decay_method=\"exponential\"\n",
    ")\n",
    "\n",
    "# Run the learned policy\n",
    "total_reward = run_learned_policy(env, agent)\n",
    "print(f\"Total Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5821cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid(env, agent, reward_across_episodes: list, epsilons_across_episodes: list) -> None:\n",
    "    env.train = False\n",
    "    total_reward_learned_policy = [run_learned_policy(env, agent) for _ in range(30)]\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Main plot\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(reward_across_episodes, 'ro')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward Value')\n",
    "    plt.title('Rewards Per Episode (Training)')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(total_reward_learned_policy, 'ro')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward Value')\n",
    "    plt.title('Rewards Per Episode (Learned Policy Evaluation)')\n",
    "    plt.grid()\n",
    "\n",
    "    # Extra plots\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(reward_across_episodes)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulative Reward Per Episode (Training)')\n",
    "    plt.title('Cumulative Reward vs Episode')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(epsilons_across_episodes)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Epsilon Values')\n",
    "    plt.title('Epsilon Decay')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_grid(env, agent, reward_across_episodes, epsilons_across_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a3c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_trading_environment = StockTradingEnvironment('./AAPL_data.csv', number_of_days_to_consider=30)\n",
    "stock_trading_environment.train = False\n",
    "obs, _ = stock_trading_environment.reset()\n",
    "terminated, truncated = False, False\n",
    "while not terminated:\n",
    "    # Convert observation to a tuple\n",
    "    obs_tuple = tuple(obs.flatten())\n",
    "    action = np.argmax(agent.q_table1.get(obs_tuple, np.zeros(env.action_space.n)) + agent.q_table2.get(obs_tuple, np.zeros(env.action_space.n)))\n",
    "    obs, reward, terminated, truncated, info = stock_trading_environment.step(action)\n",
    "\n",
    "stock_trading_environment.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba8d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(agent, 'aapl_q_learning_agent.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b416066",
   "metadata": {},
   "source": [
    "#### Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2ec473",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = load_pickle(\"aapl_q_learning_agent.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e23d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_trading_environment = StockTradingEnvironment('./AAPL_data.csv', number_of_days_to_consider=30)\n",
    "stock_trading_environment.train = False\n",
    "obs, _ = stock_trading_environment.reset()\n",
    "terminated, truncated = False, False\n",
    "while not terminated:\n",
    "    # Convert observation to a tuple\n",
    "    obs_tuple = tuple(obs.flatten())\n",
    "    action = np.argmax(agent.q_table1.get(obs_tuple, np.zeros(env.action_space.n)) + agent.q_table2.get(obs_tuple, np.zeros(env.action_space.n)))\n",
    "    obs, reward, terminated, truncated, info = stock_trading_environment.step(action)\n",
    "\n",
    "stock_trading_environment.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
