{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f1cf939",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-21 21:33:08,901 - INFO - Using GPU for training\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from src.environment import StockTradingEnvironment\n",
    "from src.utils import save_pickle, load_pickle, plot_grid\n",
    "from src.learner import q_learning_learning_loop\n",
    "import yfinance as yf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# Logging setup for better tracking of progress\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    logging.info(\"Using GPU for training\")\n",
    "else:\n",
    "    logging.info(\"Using CPU for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "169c51c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# yfinance download data\n",
    "def fetch_stock_data(symbol, start_date, end_date, output_file):\n",
    "    stock_data = yf.download(symbol, start=start_date, end=end_date)\n",
    "    stock_data['Close'] = stock_data['Adj Close']\n",
    "    \n",
    "    stock_data = stock_data.drop(columns=['Adj Close'])\n",
    "\n",
    "    stock_data.to_csv(output_file)\n",
    "        \n",
    "    return stock_data\n",
    "\n",
    "stock_data = fetch_stock_data('AAPL', '2019-06-20', '2024-06-21', 'AAPL_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd4c31",
   "metadata": {},
   "source": [
    "#### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b1cb59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.write = 0\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "        self.tree[parent] += change\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s - self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "        return idx, self.tree[idx], self.data[dataIdx]\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.alpha = alpha\n",
    "        self.capacity = capacity\n",
    "        self.epsilon = 0.01  # small amount to avoid zero priority\n",
    "\n",
    "    def add(self, error, sample):\n",
    "        p = (error + self.epsilon) ** self.alpha\n",
    "        self.tree.add(p, sample)\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        batch = []\n",
    "        idxs = []\n",
    "        segment = self.tree.total() / batch_size\n",
    "        priorities = []\n",
    "        for i in range(batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            s = random.uniform(a, b)\n",
    "            idx, p, data = self.tree.get(s)\n",
    "            batch.append(data)\n",
    "            idxs.append(idx)\n",
    "            priorities.append(p)\n",
    "        sampling_probabilities = priorities / self.tree.total()\n",
    "        is_weights = np.power(self.tree.capacity * sampling_probabilities, -beta)\n",
    "        is_weights /= is_weights.max()\n",
    "        return batch, idxs, is_weights\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = (error + self.epsilon) ** self.alpha\n",
    "        self.tree.update(idx, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e5f3ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleQLearningAgent:\n",
    "    def __init__(self, env, learning_rate, discount_factor):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.q_table1 = {}\n",
    "        self.q_table2 = {}\n",
    "\n",
    "    def step(self, state, epsilon):\n",
    "        state_index = self._get_state_index(state)\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.choice(self.env.action_space.n)\n",
    "        else:\n",
    "            q_values = self.q_table1.get(state_index, np.zeros(self.env.action_space.n)) + self.q_table2.get(state_index, np.zeros(self.env.action_space.n))\n",
    "            return np.argmax(q_values)\n",
    "\n",
    "    def update_qvalue(self, state, action, reward, next_state):\n",
    "        state_index = self._get_state_index(state)\n",
    "        next_state_index = self._get_state_index(next_state)\n",
    "        if np.random.rand() < 0.5:\n",
    "            best_next_action = np.argmax(self.q_table1.get(next_state_index, np.zeros(self.env.action_space.n)))\n",
    "            self.q_table1[state_index] = self.q_table1.get(state_index, np.zeros(self.env.action_space.n))\n",
    "            self.q_table1[state_index][action] += self.learning_rate * (\n",
    "                reward + self.discount_factor * self.q_table2.get(next_state_index, np.zeros(self.env.action_space.n))[best_next_action] - self.q_table1[state_index][action]\n",
    "            )\n",
    "        else:\n",
    "            best_next_action = np.argmax(self.q_table2.get(next_state_index, np.zeros(self.env.action_space.n)))\n",
    "            self.q_table2[state_index] = self.q_table2.get(state_index, np.zeros(self.env.action_space.n))\n",
    "            self.q_table2[state_index][action] += self.learning_rate * (\n",
    "                reward + self.discount_factor * self.q_table1.get(next_state_index, np.zeros(self.env.action_space.n))[best_next_action] - self.q_table2[state_index][action]\n",
    "            )\n",
    "\n",
    "    def _get_state_index(self, state):\n",
    "        return tuple(state.flatten())\n",
    "\n",
    "# Updated DQNAgent class with prioritized experience replay\n",
    "class DQNAgent:\n",
    "    def __init__(self, env, learning_rate, discount_factor, buffer_size=10000, batch_size=32, alpha=0.6, beta=0.4):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = PrioritizedReplayBuffer(buffer_size, alpha)\n",
    "        self.beta = beta\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = models.Sequential()\n",
    "        input_dim = np.prod(self.env.observation_space.shape)\n",
    "        model.add(layers.Dense(24, input_dim=input_dim, activation='relu'))\n",
    "        model.add(layers.Dense(24, activation='relu'))\n",
    "        model.add(layers.Dense(self.env.action_space.n, activation='linear'))\n",
    "        model.compile(optimizer=optimizers.Adam(learning_rate=self.learning_rate), loss='mse')\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        state = state.flatten().reshape(1, -1)\n",
    "        next_state = next_state.flatten().reshape(1, -1)\n",
    "        target = reward + self.discount_factor * np.amax(self.target_model.predict(next_state)[0]) if not done else reward\n",
    "        td_error = abs(target - np.amax(self.model.predict(state)[0]))\n",
    "        self.memory.add(td_error, (state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        if np.random.rand() <= epsilon:\n",
    "            return np.random.choice(self.env.action_space.n)\n",
    "        state = state.flatten().reshape(1, -1)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory.tree.data) < self.batch_size:\n",
    "            return\n",
    "        minibatch, idxs, is_weights = self.memory.sample(self.batch_size, self.beta)\n",
    "        for i, (state, action, reward, next_state, done) in enumerate(minibatch):\n",
    "            state = state.flatten().reshape(1, -1)\n",
    "            next_state = next_state.flatten().reshape(1, -1)\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.discount_factor * np.amax(self.target_model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            td_error = abs(target - target_f[0][action])\n",
    "            self.memory.update(idxs[i], td_error)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        self.update_target_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22706b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_experience(env, agent, epsilon):\n",
    "    obs, _ = env.reset()\n",
    "    terminated, truncated = False, False\n",
    "    experiences = []\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        current_action = agent.step(obs, epsilon)\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(current_action)\n",
    "        experiences.append((obs, current_action, reward, next_obs, terminated))\n",
    "        obs = next_obs\n",
    "    \n",
    "    return experiences\n",
    "\n",
    "def adaptive_learning_rate(epoch, lr):\n",
    "    if epoch % 100 == 0 and epoch:\n",
    "        return lr * 0.1\n",
    "    return lr\n",
    "\n",
    "def decay_epsilon(episode, epsilon, min_epsilon, decay_rate):\n",
    "    return max(min_epsilon, epsilon * decay_rate)\n",
    "\n",
    "def q_learning_learning_loop(env, agent, learning_rate: float, discount_factor: float, episodes: int,\n",
    "                             min_epsilon_allowed: float, initial_epsilon_value: float,\n",
    "                             buffer_size: int = 10000, batch_size: int = 32, decay_method=\"exponential\") -> tuple:\n",
    "    epsilon = initial_epsilon_value\n",
    "    epsilon_decay_factor = np.power(min_epsilon_allowed / epsilon, 1 / episodes)\n",
    "\n",
    "    reward_across_episodes = []\n",
    "    epsilons_across_episodes = []\n",
    "\n",
    "    replay_buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        experiences = collect_experience(env, agent, epsilon)\n",
    "        replay_buffer.extend(experiences)\n",
    "\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            batch = np.array(random.sample(replay_buffer, batch_size), dtype=object)\n",
    "            for b_state, b_action, b_reward, b_next_state, b_done in batch:\n",
    "                b_state = tuple(b_state.flatten())  # Convert to tuple\n",
    "                b_next_state = tuple(b_next_state.flatten())  # Convert to tuple\n",
    "                if b_done:\n",
    "                    target = b_reward\n",
    "                else:\n",
    "                    target = b_reward + discount_factor * np.max(agent.q_table1.get(b_next_state, np.zeros(env.action_space.n)) + agent.q_table2.get(b_next_state, np.zeros(env.action_space.n)))\n",
    "\n",
    "                agent.q_table1[b_state] = agent.q_table1.get(b_state, np.zeros(env.action_space.n))\n",
    "                agent.q_table1[b_state][b_action] += learning_rate * (target - agent.q_table1[b_state][b_action])\n",
    "\n",
    "        if decay_method == \"exponential\":\n",
    "            epsilon = decay_epsilon(episode, epsilon, min_epsilon_allowed, epsilon_decay_factor)\n",
    "        else:\n",
    "            epsilon = max(min_epsilon_allowed, epsilon - (initial_epsilon_value - min_epsilon_allowed) / episodes)\n",
    "        \n",
    "        reward_across_episodes.append(sum([exp[2] for exp in experiences]))\n",
    "        epsilons_across_episodes.append(epsilon)\n",
    "\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            logging.info(f\"Episode {episode + 1}/{episodes} - Reward: {reward_across_episodes[-1]} - Epsilon: {epsilon}\")\n",
    "\n",
    "    logging.info(\"Trained Q-Table: %s\", agent.q_table1)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(reward_across_episodes, label='Rewards')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.title('Rewards over Episodes')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epsilons_across_episodes, label='Epsilon')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Epsilon')\n",
    "    plt.title('Epsilon Decay over Episodes')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return agent, reward_across_episodes, epsilons_across_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca06c6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: You can adjust the parameter 'number_of_days_to_consider'\n",
    "\n",
    "env = stock_trading_environment = StockTradingEnvironment('./AAPL_data.csv', number_of_days_to_consider=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4d38c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-21 21:39:09,098 - INFO - Episode 100/50000 - Reward: -3207.4012237801294 - Epsilon: 0.9908319448927725\n",
      "2024-06-21 21:41:08,912 - INFO - Episode 200/50000 - Reward: -3028.881331748305 - Epsilon: 0.9817479430199942\n",
      "2024-06-21 21:43:08,708 - INFO - Episode 300/50000 - Reward: -3311.239223120432 - Epsilon: 0.9727472237769801\n",
      "2024-06-21 21:45:08,405 - INFO - Episode 400/50000 - Reward: -3366.85274712864 - Epsilon: 0.9638290236239909\n",
      "2024-06-21 21:47:08,090 - INFO - Episode 500/50000 - Reward: -3129.64920970155 - Epsilon: 0.954992586021461\n",
      "2024-06-21 21:49:08,221 - INFO - Episode 600/50000 - Reward: -3193.5378886824783 - Epsilon: 0.9462371613658228\n",
      "2024-06-21 21:51:08,255 - INFO - Episode 700/50000 - Reward: -3042.1346632242316 - Epsilon: 0.937562006925914\n",
      "2024-06-21 21:53:08,754 - INFO - Episode 800/50000 - Reward: -3144.7012012828 - Epsilon: 0.9289663867799748\n",
      "2024-06-21 21:55:09,121 - INFO - Episode 900/50000 - Reward: -3378.3081238929262 - Epsilon: 0.9204495717532143\n",
      "2024-06-21 21:57:10,269 - INFO - Episode 1000/50000 - Reward: -3514.966917855341 - Epsilon: 0.9120108393559571\n",
      "2024-06-21 21:59:11,134 - INFO - Episode 1100/50000 - Reward: -3340.9487685762665 - Epsilon: 0.9036494737223535\n",
      "2024-06-21 22:01:12,313 - INFO - Episode 1200/50000 - Reward: -3457.389861845662 - Epsilon: 0.8953647655496506\n",
      "2024-06-21 22:03:13,505 - INFO - Episode 1300/50000 - Reward: -3538.8965060785613 - Epsilon: 0.8871560120380219\n",
      "2024-06-21 22:05:14,418 - INFO - Episode 1400/50000 - Reward: -3637.6019380491057 - Epsilon: 0.8790225168309495\n",
      "2024-06-21 22:07:16,067 - INFO - Episode 1500/50000 - Reward: -3604.628320660853 - Epsilon: 0.8709635899561496\n",
      "2024-06-21 22:09:17,370 - INFO - Episode 1600/50000 - Reward: -3583.856115940103 - Epsilon: 0.8629785477670433\n",
      "2024-06-21 22:11:18,990 - INFO - Episode 1700/50000 - Reward: -3574.388746048714 - Epsilon: 0.8550667128847602\n",
      "2024-06-21 22:13:20,908 - INFO - Episode 1800/50000 - Reward: -3356.5130646934085 - Epsilon: 0.847227414140677\n",
      "2024-06-21 22:15:22,664 - INFO - Episode 1900/50000 - Reward: -3346.2575601577446 - Epsilon: 0.8394599865194816\n",
      "2024-06-21 22:17:24,745 - INFO - Episode 2000/50000 - Reward: -3413.5871963421023 - Epsilon: 0.831763771102759\n",
      "2024-06-21 22:19:27,335 - INFO - Episode 2100/50000 - Reward: -3384.91648008113 - Epsilon: 0.824138115013094\n",
      "2024-06-21 22:21:30,286 - INFO - Episode 2200/50000 - Reward: -3227.4627656273838 - Epsilon: 0.8165823713586867\n",
      "2024-06-21 22:23:32,345 - INFO - Episode 2300/50000 - Reward: -3291.7083669985095 - Epsilon: 0.8090958991784798\n",
      "2024-06-21 22:25:34,838 - INFO - Episode 2400/50000 - Reward: -3527.619284062463 - Epsilon: 0.8016780633877798\n",
      "2024-06-21 22:27:39,122 - INFO - Episode 2500/50000 - Reward: -3571.6449886602377 - Epsilon: 0.7943282347243855\n",
      "2024-06-21 22:29:44,281 - INFO - Episode 2600/50000 - Reward: -3484.9697831242743 - Epsilon: 0.7870457896952061\n",
      "2024-06-21 22:31:49,170 - INFO - Episode 2700/50000 - Reward: -3818.3497253413725 - Epsilon: 0.779830110523369\n",
      "2024-06-21 22:33:53,210 - INFO - Episode 2800/50000 - Reward: -3632.282349485513 - Epsilon: 0.7726805850958163\n",
      "2024-06-21 22:35:58,474 - INFO - Episode 2900/50000 - Reward: -3619.6316205861344 - Epsilon: 0.7655966069113732\n",
      "2024-06-21 22:38:03,690 - INFO - Episode 3000/50000 - Reward: -3650.450773114656 - Epsilon: 0.7585775750293033\n",
      "2024-06-21 22:40:09,167 - INFO - Episode 3100/50000 - Reward: -3727.429780723365 - Epsilon: 0.7516228940183277\n",
      "2024-06-21 22:42:16,175 - INFO - Episode 3200/50000 - Reward: -3758.0691807689477 - Epsilon: 0.7447319739061141\n",
      "2024-06-21 22:44:23,448 - INFO - Episode 3300/50000 - Reward: -3601.2938010717958 - Epsilon: 0.7379042301292285\n",
      "2024-06-21 22:46:34,721 - INFO - Episode 3400/50000 - Reward: -4035.206691972851 - Epsilon: 0.7311390834835477\n",
      "2024-06-21 22:48:44,662 - INFO - Episode 3500/50000 - Reward: -3951.518126834815 - Epsilon: 0.7244359600751226\n",
      "2024-06-21 22:51:03,037 - INFO - Episode 3600/50000 - Reward: -3671.4161663379004 - Epsilon: 0.7177942912714969\n",
      "2024-06-21 22:53:17,683 - INFO - Episode 3700/50000 - Reward: -3817.2042041374725 - Epsilon: 0.7112135136534666\n",
      "2024-06-21 22:55:50,448 - INFO - Episode 3800/50000 - Reward: -3871.1564488222666 - Epsilon: 0.7046930689672868\n",
      "2024-06-21 22:58:08,184 - INFO - Episode 3900/50000 - Reward: -3923.32033140505 - Epsilon: 0.6982324040773131\n",
      "2024-06-21 23:00:30,444 - INFO - Episode 4000/50000 - Reward: -3833.4415539885767 - Epsilon: 0.6918309709190805\n",
      "2024-06-21 23:03:02,312 - INFO - Episode 4100/50000 - Reward: -3927.2968219593804 - Epsilon: 0.685488226452808\n",
      "2024-06-21 23:06:14,473 - INFO - Episode 4200/50000 - Reward: -3998.7474281092054 - Epsilon: 0.6792036326173337\n",
      "2024-06-21 23:10:00,684 - INFO - Episode 4300/50000 - Reward: -3903.9897839383793 - Epsilon: 0.6729766562844688\n",
      "2024-06-21 23:13:19,592 - INFO - Episode 4400/50000 - Reward: -4255.918025767182 - Epsilon: 0.6668067692137751\n",
      "2024-06-21 23:17:21,125 - INFO - Episode 4500/50000 - Reward: -3992.014717703783 - Epsilon: 0.6606934480077508\n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x1074e2860>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/beckett/miniconda3/envs/tradeEnv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "# Example with a higher batch size of 256\n",
    "agent = DoubleQLearningAgent(env, learning_rate=0.001, discount_factor=0.95)\n",
    "\n",
    "agent, reward_across_episodes, epsilons_across_episodes = q_learning_learning_loop(\n",
    "    env,\n",
    "    agent,\n",
    "    learning_rate=0.001,\n",
    "    discount_factor=0.95,\n",
    "    episodes=50000,\n",
    "    min_epsilon_allowed=0.01,\n",
    "    initial_epsilon_value=1,\n",
    "    buffer_size=100000,\n",
    "    batch_size=256,  # Increased batch size\n",
    "    decay_method=\"exponential\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed4414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_learned_policy(env, agent):\n",
    "    obs, _ = env.reset()\n",
    "    terminated, truncated = False, False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        # Convert the observation to a tuple to use as a key in the Q-tables\n",
    "        obs_tuple = tuple(obs.flatten())\n",
    "        action = np.argmax(agent.q_table1.get(obs_tuple, np.zeros(env.action_space.n)) + agent.q_table2.get(obs_tuple, np.zeros(env.action_space.n)))\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "    logging.info(\"Total Reward: %d, Steps: %d\", total_reward, steps)\n",
    "    return total_reward\n",
    "\n",
    "total_reward = run_learned_policy(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5821cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid(env, agent, reward_across_episodes: list, epsilons_across_episodes: list) -> None:\n",
    "    env.train = False\n",
    "    total_reward_learned_policy = [run_learned_policy(env, agent) for _ in range(30)]\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Main plot\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(reward_across_episodes, 'ro')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward Value')\n",
    "    plt.title('Rewards Per Episode (Training)')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(total_reward_learned_policy, 'ro')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward Value')\n",
    "    plt.title('Rewards Per Episode (Learned Policy Evaluation)')\n",
    "    plt.grid()\n",
    "\n",
    "    # Extra plots\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(reward_across_episodes)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulative Reward Per Episode (Training)')\n",
    "    plt.title('Cumulative Reward vs Episode')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(epsilons_across_episodes)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Epsilon Values')\n",
    "    plt.title('Epsilon Decay')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_grid(stock_trading_environment, agent, reward_across_episodes, epsilons_across_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a3c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_trading_environment = StockTradingEnvironment('./AAPL_data.csv', number_of_days_to_consider=30)\n",
    "stock_trading_environment.train = False\n",
    "obs, _ = stock_trading_environment.reset()\n",
    "terminated, truncated = False, False\n",
    "while not terminated:\n",
    "    # Convert observation to a tuple\n",
    "    obs_tuple = tuple(obs.flatten())\n",
    "    action = np.argmax(agent.q_table1.get(obs_tuple, np.zeros(env.action_space.n)) + agent.q_table2.get(obs_tuple, np.zeros(env.action_space.n)))\n",
    "    obs, reward, terminated, truncated, info = stock_trading_environment.step(action)\n",
    "\n",
    "stock_trading_environment.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba8d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(agent, 'aapl_q_learning_agent.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b416066",
   "metadata": {},
   "source": [
    "#### Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2ec473",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = load_pickle(\"aapl_q_learning_agent.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e23d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_trading_environment = StockTradingEnvironment('./AAPL_data.csv', number_of_days_to_consider=30)\n",
    "stock_trading_environment.train = False\n",
    "obs, _ = stock_trading_environment.reset()\n",
    "terminated, truncated = False, False\n",
    "while not terminated:\n",
    "    # Convert observation to a tuple\n",
    "    obs_tuple = tuple(obs.flatten())\n",
    "    action = np.argmax(agent.q_table1.get(obs_tuple, np.zeros(env.action_space.n)) + agent.q_table2.get(obs_tuple, np.zeros(env.action_space.n)))\n",
    "    obs, reward, terminated, truncated, info = stock_trading_environment.step(action)\n",
    "\n",
    "stock_trading_environment.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
